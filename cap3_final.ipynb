{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "patent-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, classification_report, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, AlphaDropout, BatchNormalization, Add, concatenate\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, AveragePooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.initializers import LecunNormal\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "plt.rcParams.update({'font.size':15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sonic-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all datasets\n",
    "test_feat = pd.read_csv('data/test_features.csv')\n",
    "train_drug = pd.read_csv('data/train_drug.csv')\n",
    "train_feat = pd.read_csv('data/train_features.csv')\n",
    "train_targets_nonscored = pd.read_csv('data/train_targets_nonscored.csv')\n",
    "train_targets_scored = pd.read_csv('data/train_targets_scored.csv')\n",
    "\n",
    "# Categorical to numerical\n",
    "train_feat.cp_time = train_feat.cp_time.map({24: 1, 48: 2, 72: 3}).astype(int)\n",
    "train_feat = pd.get_dummies(train_feat, columns=['cp_type'], prefix=\"cp_type\", drop_first=True)\n",
    "train_feat = pd.get_dummies(train_feat, columns=[\"cp_dose\"], prefix=\"cp_dose\", drop_first=True)\n",
    "\n",
    "# Actually this test_features.csv has no targets and is used for making prediction and submitting as submission.csv\n",
    "# for competition scoring purpose. If I am not going to submit, then it's useless.\n",
    "test_feat.cp_time = test_feat.cp_time.map({24: 1, 48: 2, 72: 3}).astype(int)\n",
    "test_feat = pd.get_dummies(test_feat, columns=[\"cp_type\"], prefix=\"cp_type\", drop_first=True)\n",
    "test_feat = pd.get_dummies(test_feat, columns=[\"cp_dose\"], prefix=\"cp_dose\", drop_first=True)\n",
    "\n",
    "# Get X, y using train datasets\n",
    "X = train_feat.drop('sig_id', axis=1).values\n",
    "y = train_targets_scored.drop('sig_id', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "whole-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 70\n",
    "pca70 = PCA(n_components=n_components)\n",
    "X_pca70 = pca70.fit_transform(X)\n",
    "\n",
    "# first split into train_all and test, latter is used as holdout data.\n",
    "X_pca70_train_all, X_pca70_test, y_train_all, y_test = train_test_split(X_pca70, y, test_size=0.20, shuffle=True, random_state=1)\n",
    "\n",
    "# then split train_all to train and val, the former is used as train data, and the latter is used as validation data.\n",
    "X_pca70_train, X_pca70_val, y_train, y_val = train_test_split(X_pca70_train_all, y_train_all, test_size=0.20, random_state=1)\n",
    "\n",
    "# X_pca70_train_reshaped = X_pca70_train.reshape(X_pca70_train.shape[0], X_pca70_train.shape[1], 1)\n",
    "# X_pca70_val_reshaped = X_pca70_val.reshape(X_pca70_val.shape[0], X_pca70_val.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aggressive-patient",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15240, 70), (3811, 70), (4763, 70), (15240, 206), (3811, 206), (4763, 206))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca70_train.shape, X_pca70_val.shape, X_pca70_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chinese-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to define the log-loss metric required by the competition, see formula here:\n",
    "# https://www.kaggle.com/c/lish-moa/overview/evaluation\n",
    "def logloss(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, 0.001, 0.999)\n",
    "    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "scheduled-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return log_loss(y, y_pred) / y.shape[1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "sapphire-tragedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_215 (Dense)            (None, 64)                4544      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_157 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_158 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 206)               26574     \n",
      "=================================================================\n",
      "Total params: 40,206\n",
      "Trainable params: 39,822\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2376b6ee27460abc75a0f4402eb374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0batch [00:00, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "477/477 [==============================] - 3s 4ms/step - loss: 0.4147 - logloss: 0.4140 - val_loss: 0.0235 - val_logloss: 0.0209\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.19036, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 2/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0236 - logloss: 0.0209 - val_loss: 0.0213 - val_logloss: 0.0184\n",
      "\n",
      "Epoch 00002: loss improved from 0.19036 to 0.02303, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 3/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0220 - logloss: 0.0190 - val_loss: 0.0207 - val_logloss: 0.0176\n",
      "\n",
      "Epoch 00003: loss improved from 0.02303 to 0.02179, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 4/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0212 - logloss: 0.0181 - val_loss: 0.0204 - val_logloss: 0.0172\n",
      "\n",
      "Epoch 00004: loss improved from 0.02179 to 0.02116, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 5/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0208 - logloss: 0.0177 - val_loss: 0.0201 - val_logloss: 0.0168\n",
      "\n",
      "Epoch 00005: loss improved from 0.02116 to 0.02067, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 6/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0202 - logloss: 0.0171 - val_loss: 0.0199 - val_logloss: 0.0167\n",
      "\n",
      "Epoch 00006: loss improved from 0.02067 to 0.02032, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 7/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0201 - logloss: 0.0170 - val_loss: 0.0198 - val_logloss: 0.0165\n",
      "\n",
      "Epoch 00007: loss improved from 0.02032 to 0.02005, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 8/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0199 - logloss: 0.0168 - val_loss: 0.0197 - val_logloss: 0.0164\n",
      "\n",
      "Epoch 00008: loss improved from 0.02005 to 0.01978, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 9/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0197 - logloss: 0.0165 - val_loss: 0.0196 - val_logloss: 0.0163\n",
      "\n",
      "Epoch 00009: loss improved from 0.01978 to 0.01953, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 10/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0194 - logloss: 0.0162 - val_loss: 0.0195 - val_logloss: 0.0162\n",
      "\n",
      "Epoch 00010: loss improved from 0.01953 to 0.01938, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 11/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0191 - logloss: 0.0159 - val_loss: 0.0195 - val_logloss: 0.0161\n",
      "\n",
      "Epoch 00011: loss improved from 0.01938 to 0.01918, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 12/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0188 - logloss: 0.0156 - val_loss: 0.0194 - val_logloss: 0.0161\n",
      "\n",
      "Epoch 00012: loss improved from 0.01918 to 0.01905, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 13/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0190 - logloss: 0.0158 - val_loss: 0.0194 - val_logloss: 0.0160\n",
      "\n",
      "Epoch 00013: loss improved from 0.01905 to 0.01895, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 14/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0186 - logloss: 0.0154 - val_loss: 0.0194 - val_logloss: 0.0160\n",
      "\n",
      "Epoch 00014: loss improved from 0.01895 to 0.01876, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 15/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0186 - logloss: 0.0155 - val_loss: 0.0193 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00015: loss improved from 0.01876 to 0.01870, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 16/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0184 - logloss: 0.0152 - val_loss: 0.0194 - val_logloss: 0.0160\n",
      "\n",
      "Epoch 00016: loss improved from 0.01870 to 0.01854, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 17/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0184 - logloss: 0.0153 - val_loss: 0.0193 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00017: loss improved from 0.01854 to 0.01849, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 18/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0185 - logloss: 0.0153 - val_loss: 0.0193 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00018: loss improved from 0.01849 to 0.01838, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 19/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0181 - logloss: 0.0149 - val_loss: 0.0193 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00019: loss improved from 0.01838 to 0.01832, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 20/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0183 - logloss: 0.0151 - val_loss: 0.0193 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00020: loss improved from 0.01832 to 0.01823, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 21/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0179 - logloss: 0.0147 - val_loss: 0.0193 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00021: loss improved from 0.01823 to 0.01814, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 22/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0180 - logloss: 0.0148 - val_loss: 0.0194 - val_logloss: 0.0160\n",
      "\n",
      "Epoch 00022: loss improved from 0.01814 to 0.01811, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 23/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0179 - logloss: 0.0147 - val_loss: 0.0194 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00023: loss improved from 0.01811 to 0.01803, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 24/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0178 - logloss: 0.0146 - val_loss: 0.0194 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00024: loss improved from 0.01803 to 0.01795, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 25/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0179 - logloss: 0.0147 - val_loss: 0.0194 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00025: loss improved from 0.01795 to 0.01789, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 26/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0176 - logloss: 0.0144 - val_loss: 0.0194 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00026: loss improved from 0.01789 to 0.01783, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 27/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0175 - logloss: 0.0143 - val_loss: 0.0194 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00027: loss improved from 0.01783 to 0.01778, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 28/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0176 - logloss: 0.0144 - val_loss: 0.0194 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00028: loss improved from 0.01778 to 0.01768, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 29/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0176 - logloss: 0.0144 - val_loss: 0.0194 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00029: loss did not improve from 0.01768\n",
      "Epoch 30/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0175 - logloss: 0.0143 - val_loss: 0.0194 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00030: loss improved from 0.01768 to 0.01760, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 31/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0175 - logloss: 0.0143 - val_loss: 0.0195 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00031: loss improved from 0.01760 to 0.01756, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 32/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0174 - logloss: 0.0141 - val_loss: 0.0194 - val_logloss: 0.0158\n",
      "\n",
      "Epoch 00032: loss improved from 0.01756 to 0.01753, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 33/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0174 - logloss: 0.0142 - val_loss: 0.0194 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00033: loss improved from 0.01753 to 0.01753, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 34/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0173 - logloss: 0.0140 - val_loss: 0.0195 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00034: loss improved from 0.01753 to 0.01748, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 35/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0173 - logloss: 0.0140 - val_loss: 0.0195 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00035: loss improved from 0.01748 to 0.01735, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 36/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0172 - logloss: 0.0139 - val_loss: 0.0195 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.01735\n",
      "Epoch 37/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0196 - val_logloss: 0.0160\n",
      "\n",
      "Epoch 00037: loss improved from 0.01735 to 0.01730, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 38/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0174 - logloss: 0.0141 - val_loss: 0.0195 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.01730\n",
      "Epoch 39/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0195 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00039: loss improved from 0.01730 to 0.01726, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 40/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0172 - logloss: 0.0140 - val_loss: 0.0195 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.01726\n",
      "Epoch 41/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0170 - logloss: 0.0137 - val_loss: 0.0196 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00041: loss improved from 0.01726 to 0.01720, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 42/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0196 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00042: loss improved from 0.01720 to 0.01716, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 43/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0168 - logloss: 0.0135 - val_loss: 0.0196 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.01716\n",
      "Epoch 44/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0168 - logloss: 0.0135 - val_loss: 0.0196 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00044: loss improved from 0.01716 to 0.01710, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 45/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0170 - logloss: 0.0137 - val_loss: 0.0196 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.01710\n",
      "Epoch 46/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0195 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00046: loss improved from 0.01710 to 0.01704, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 47/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0170 - logloss: 0.0137 - val_loss: 0.0196 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.01704\n",
      "Epoch 48/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0168 - logloss: 0.0135 - val_loss: 0.0196 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00048: loss improved from 0.01704 to 0.01703, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 49/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0169 - logloss: 0.0136 - val_loss: 0.0197 - val_logloss: 0.0160\n",
      "\n",
      "Epoch 00049: loss improved from 0.01703 to 0.01703, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Epoch 50/50\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0167 - logloss: 0.0133 - val_loss: 0.0196 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00050: loss improved from 0.01703 to 0.01695, saving model to models/mlp_base\n",
      "INFO:tensorflow:Assets written to: models/mlp_base/assets\n",
      "Validation log-loss:  0.01594577543437481\n",
      "INFO:tensorflow:Assets written to: models/mlp_base_complete/assets\n",
      "Total running time: 2.5 minutes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "# Build a basic MLP model first\n",
    "def build_model(n_cols):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(128, activation='relu' ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(206, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=0.001, decay=0.001/200), loss=BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n",
    "\n",
    "    return model \n",
    "\n",
    "n_cols = X_pca70_train.shape[1]\n",
    "mlp_base = build_model(n_cols)\n",
    "mlp_base.summary()\n",
    "\n",
    "earlystp_mlp_base = EarlyStopping(monitor='loss', patience=5)\n",
    "filepath = 'models/mlp_base'\n",
    "checkpoint_mlp_base = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history_mlp_base = mlp_base.fit(\n",
    "    X_pca70_train, y_train, batch_size=32,\n",
    "    validation_data=(X_pca70_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=[checkpoint_mlp_base, earlystp_mlp_base, TqdmCallback()])\n",
    "\n",
    "score = mlp_base.evaluate(X_pca70_val, y_val, verbose=0)\n",
    "print('Validation log-loss: ', score[1])\n",
    "\n",
    "mlp_base.save('models/mlp_base_complete')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "[ax.plot(history_mlp_base.history[i], label=i) for i in ['logloss', 'val_logloss']] #history_alexnet.history.keys()]\n",
    "ax.set_xlabel('Epochs', fontsize=15)\n",
    "ax.set_ylabel('Log-Loss', fontsize=15)\n",
    "ax.set_title('MLP Base Model with PCA (n_components=70)', fontsize=20)\n",
    "plt.legend(fontsize=13, loc='best')\n",
    "plt.savefig('images/mlp_base.jpg')\n",
    "\n",
    "\n",
    "# calculate the running time\n",
    "end = time.time()\n",
    "print(\"Total running time:\", round((end - start)/60, 1), 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "accepting-lloyd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params testing:                                       \n",
      "{'activation': 'relu', 'choice': {'layers': 'two'}, 'dropout1': 0.35131459808890775, 'dropout2': 0.16874552331862575, 'units1': 218.57716638255835, 'units2': 134.68070182111856}\n",
      "Log-Loss:                                             \n",
      "0.01640928164124489                                   \n",
      "Params testing:                                                                  \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.30227136160939916, 'layers': 'three', 'units3': 200.23773954728017}, 'dropout1': 0.18070490325464045, 'dropout2': 0.3593747743956156, 'units1': 158.15102305816262, 'units2': 102.7885436666957}\n",
      "Log-Loss:                                                                        \n",
      "0.015897998586297035                                                             \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.15687800410385783, 'layers': 'three', 'units3': 182.4458958784395}, 'dropout1': 0.39296268084715924, 'dropout2': 0.38719493474095046, 'units1': 188.26688291532054, 'units2': 95.23976696809765}\n",
      "Log-Loss:                                                                         \n",
      "0.015733866021037102                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'layers': 'two'}, 'dropout1': 0.394884914308796, 'dropout2': 0.1700450300393136, 'units1': 79.48643315140149, 'units2': 116.58053103803977}\n",
      "Log-Loss:                                                                         \n",
      "0.015913181006908417                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.3583417113182996, 'layers': 'three', 'units3': 208.36659526418708}, 'dropout1': 0.30835847126484983, 'dropout2': 0.1073814281573114, 'units1': 128.503742854704, 'units2': 238.29235441696866}\n",
      "Log-Loss:                                                                         \n",
      "0.015885543078184128                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.1711999673171894, 'layers': 'three', 'units3': 131.42771650271322}, 'dropout1': 0.22428935175545595, 'dropout2': 0.3827848094717198, 'units1': 70.18873098605306, 'units2': 118.74165903753614}\n",
      "Log-Loss:                                                                         \n",
      "0.015752049162983894                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.3610662023387574, 'layers': 'three', 'units3': 254.93527415640835}, 'dropout1': 0.25143085017824063, 'dropout2': 0.15716131610391149, 'units1': 198.86816929692418, 'units2': 143.03476367923807}\n",
      "Log-Loss:                                                                         \n",
      "0.01591402292251587                                                               \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.3129114875455766, 'layers': 'three', 'units3': 128.54687337737266}, 'dropout1': 0.20556976973191218, 'dropout2': 0.21143114650160028, 'units1': 126.15410198853311, 'units2': 195.31564418548788}\n",
      "Log-Loss:                                                                         \n",
      "0.015834424644708633                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'layers': 'two'}, 'dropout1': 0.382573937478881, 'dropout2': 0.3010076761586631, 'units1': 206.7972472451507, 'units2': 64.24023588041915}\n",
      "Log-Loss:                                                                         \n",
      "0.015750670805573463                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.36120727003601927, 'layers': 'three', 'units3': 107.45437562864234}, 'dropout1': 0.2911591648842913, 'dropout2': 0.32225930038022466, 'units1': 149.39266373087509, 'units2': 78.60750734208945}\n",
      "Log-Loss:                                                                         \n",
      "0.015759343281388283                                                              \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.3565666368784167, 'layers': 'three', 'units3': 189.99814866596057}, 'dropout1': 0.279181230287785, 'dropout2': 0.37319361200338885, 'units1': 214.75983001620523, 'units2': 136.46354788087774}\n",
      "Log-Loss:                                                                          \n",
      "0.015816248953342438                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'layers': 'two'}, 'dropout1': 0.19852683163629622, 'dropout2': 0.3768712787616061, 'units1': 168.89994334708956, 'units2': 249.19532611184502}\n",
      "Log-Loss:                                                                          \n",
      "0.0164960864931345                                                                 \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'layers': 'two'}, 'dropout1': 0.1147460958671242, 'dropout2': 0.12308039547549632, 'units1': 65.71104408502944, 'units2': 140.50231554321277}\n",
      "Log-Loss:                                                                          \n",
      "0.016323272138834                                                                  \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'layers': 'two'}, 'dropout1': 0.3255317707100398, 'dropout2': 0.348188434787026, 'units1': 188.17450361896908, 'units2': 184.23104896764553}\n",
      "Log-Loss:                                                                          \n",
      "0.016261395066976547                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.20010299958221062, 'layers': 'three', 'units3': 134.0232096732122}, 'dropout1': 0.3520963669484998, 'dropout2': 0.1729596917778004, 'units1': 138.24560293905475, 'units2': 158.83605264773234}\n",
      "Log-Loss:                                                                          \n",
      "0.01571311056613922                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.2366113771337187, 'layers': 'three', 'units3': 167.3705158361636}, 'dropout1': 0.15380555093589382, 'dropout2': 0.2012608127523049, 'units1': 155.40983266006498, 'units2': 97.13673022455295}\n",
      "Log-Loss:                                                                         \n",
      "0.016081979498267174                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'layers': 'two'}, 'dropout1': 0.14141078835069504, 'dropout2': 0.33424837854575007, 'units1': 219.0717987485424, 'units2': 247.2992508817876}\n",
      "Log-Loss:                                                                         \n",
      "0.016784142702817917                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.3695837876512893, 'layers': 'three', 'units3': 150.19732347717323}, 'dropout1': 0.25956755572651263, 'dropout2': 0.159554724554972, 'units1': 224.122668710896, 'units2': 64.35330433443468}\n",
      "Log-Loss:                                                                         \n",
      "0.015900621190667152                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.3953225685241515, 'layers': 'three', 'units3': 122.21270041807816}, 'dropout1': 0.24027123798082653, 'dropout2': 0.2884991661096422, 'units1': 178.0279581180212, 'units2': 90.40750898494576}\n",
      "Log-Loss:                                                                         \n",
      "0.015798788517713547                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'layers': 'two'}, 'dropout1': 0.34038410582071865, 'dropout2': 0.1321997554828779, 'units1': 64.01923429154911, 'units2': 85.01245334678316}\n",
      "Log-Loss:                                                                         \n",
      "0.01588440127670765                                                               \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.1320582137936235, 'layers': 'three', 'units3': 68.11492611888399}, 'dropout1': 0.37057211627381254, 'dropout2': 0.23667423841065202, 'units1': 246.6155669703731, 'units2': 175.060504753778}\n",
      "Log-Loss:                                                                         \n",
      "0.015803644433617592                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.1966606113230665, 'layers': 'three', 'units3': 86.00752350624953}, 'dropout1': 0.3604746241751638, 'dropout2': 0.26029316742973396, 'units1': 98.11817594714915, 'units2': 214.29492797414733}\n",
      "Log-Loss:                                                                         \n",
      "0.015602216124534607                                                              \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.21322830613156846, 'layers': 'three', 'units3': 78.09048328632946}, 'dropout1': 0.3549543945809325, 'dropout2': 0.24900071174831814, 'units1': 95.87844754326156, 'units2': 216.77804673070898}\n",
      "Log-Loss:                                                                          \n",
      "0.01572396606206894                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.19953478926984733, 'layers': 'three', 'units3': 92.63389376482483}, 'dropout1': 0.3272870008472601, 'dropout2': 0.28185843010620565, 'units1': 109.4664705549263, 'units2': 210.22839661433613}\n",
      "Log-Loss:                                                                          \n",
      "0.015672005712985992                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.10178621070219032, 'layers': 'three', 'units3': 93.52902188553409}, 'dropout1': 0.3150314158098456, 'dropout2': 0.2751376229230888, 'units1': 108.22647214901409, 'units2': 218.32566210174048}\n",
      "Log-Loss:                                                                          \n",
      "0.015762830153107643                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.27164305667093197, 'layers': 'three', 'units3': 87.12170623854863}, 'dropout1': 0.28688559822243687, 'dropout2': 0.22420794198738547, 'units1': 102.5513503341282, 'units2': 209.73276801479366}\n",
      "Log-Loss:                                                                          \n",
      "0.015699272975325584                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.180373644646158, 'layers': 'three', 'units3': 64.61344900558485}, 'dropout1': 0.3298490606411872, 'dropout2': 0.2688109826299132, 'units1': 89.1492263179752, 'units2': 233.0727677557754}\n",
      "Log-Loss:                                                                          \n",
      "0.01573348231613636                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.10056565901130556, 'layers': 'three', 'units3': 104.17403961410753}, 'dropout1': 0.37270809946757777, 'dropout2': 0.3109483298265163, 'units1': 116.55850528325674, 'units2': 198.53579077201553}\n",
      "Log-Loss:                                                                          \n",
      "0.01569649949669838                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.25328558653627997, 'layers': 'three', 'units3': 244.93152240106934}, 'dropout1': 0.3020072479025377, 'dropout2': 0.25710197100706594, 'units1': 83.10634423226588, 'units2': 158.56546932303115}\n",
      "Log-Loss:                                                                          \n",
      "0.01576787419617176                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.2184342131900983, 'layers': 'three', 'units3': 109.81319031884091}, 'dropout1': 0.2729957038637772, 'dropout2': 0.1901530544914255, 'units1': 107.94545551159025, 'units2': 231.6972319905773}\n",
      "Log-Loss:                                                                          \n",
      "0.015722474083304405                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.13130012131611765, 'layers': 'three', 'units3': 66.61513129565549}, 'dropout1': 0.36107726247934213, 'dropout2': 0.29881496643056904, 'units1': 140.96999970907842, 'units2': 183.47396172796914}\n",
      "Log-Loss:                                                                          \n",
      "0.01574014313519001                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.275234084988804, 'layers': 'three', 'units3': 153.82134784022068}, 'dropout1': 0.3411290750989118, 'dropout2': 0.23984849569838876, 'units1': 127.26658495298413, 'units2': 202.02168168166932}\n",
      "Log-Loss:                                                                          \n",
      "0.01563778705894947                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.2910461549528505, 'layers': 'three', 'units3': 147.68017508702778}, 'dropout1': 0.39727016981747093, 'dropout2': 0.23520306374869404, 'units1': 123.30971763218076, 'units2': 173.42234684463892}\n",
      "Log-Loss:                                                                          \n",
      "0.015712616965174675                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.25378866100358977, 'layers': 'three', 'units3': 227.4308135745873}, 'dropout1': 0.39409563214516613, 'dropout2': 0.2562042398885981, 'units1': 170.98332218751437, 'units2': 203.5308229348018}\n",
      "Log-Loss:                                                                          \n",
      "0.015796398743987083                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'layers': 'two'}, 'dropout1': 0.34229365909008674, 'dropout2': 0.18530011561872273, 'units1': 76.4285525699073, 'units2': 220.49682024505148}\n",
      "Log-Loss:                                                                          \n",
      "0.016308113932609558                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.32541189125095554, 'layers': 'three', 'units3': 165.51271305572538}, 'dropout1': 0.37852723044488784, 'dropout2': 0.21729384100246932, 'units1': 134.22875955704404, 'units2': 254.7354081817923}\n",
      "Log-Loss:                                                                          \n",
      "0.015574329532682896                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.328778858263417, 'layers': 'three', 'units3': 170.26622584270075}, 'dropout1': 0.37982699934318426, 'dropout2': 0.2171953453929174, 'units1': 144.4276368435815, 'units2': 255.2652953103622}\n",
      "Log-Loss:                                                                          \n",
      "0.01574334129691124                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.32209368329716237, 'layers': 'three', 'units3': 210.59720368313688}, 'dropout1': 0.23408185658082822, 'dropout2': 0.39959261566644333, 'units1': 91.66222605335916, 'units2': 244.44334514119242}\n",
      "Log-Loss:                                                                          \n",
      "0.0157157089561224                                                                 \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'layers': 'two'}, 'dropout1': 0.39923113809247934, 'dropout2': 0.1434952338633645, 'units1': 137.60992267409932, 'units2': 122.66465473044742}\n",
      "Log-Loss:                                                                          \n",
      "0.01624148339033127                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.2943283658205844, 'layers': 'three', 'units3': 175.2252120665106}, 'dropout1': 0.2632343175126565, 'dropout2': 0.20499476584158566, 'units1': 158.0124287077061, 'units2': 228.25592597492306}\n",
      "Log-Loss:                                                                          \n",
      "0.01591978408396244                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.38644940895820545, 'layers': 'three', 'units3': 192.73123116999275}, 'dropout1': 0.3005400274644459, 'dropout2': 0.3388419753812997, 'units1': 235.98889747590965, 'units2': 187.58760146183377}\n",
      "Log-Loss:                                                                          \n",
      "0.015847545117139816                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.14898351680852212, 'layers': 'three', 'units3': 223.40683541236814}, 'dropout1': 0.19979199925282348, 'dropout2': 0.10592473324954327, 'units1': 116.11822765350209, 'units2': 239.24132715163623}\n",
      "Log-Loss:                                                                          \n",
      "0.016625577583909035                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'layers': 'two'}, 'dropout1': 0.175190923712992, 'dropout2': 0.1810255088820471, 'units1': 194.5631812094287, 'units2': 253.62696435138443}\n",
      "Log-Loss:                                                                          \n",
      "0.01700873300433159                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.3409132715279559, 'layers': 'three', 'units3': 140.2564812870675}, 'dropout1': 0.10585018892512546, 'dropout2': 0.3632135193261317, 'units1': 166.498795509662, 'units2': 149.88449790449897}\n",
      "Log-Loss:                                                                          \n",
      "0.015973201021552086                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.23617489221844584, 'layers': 'three', 'units3': 117.87289099340856}, 'dropout1': 0.2166775794800817, 'dropout2': 0.3181025817399954, 'units1': 77.69323948151356, 'units2': 108.1946269663302}\n",
      "Log-Loss:                                                                          \n",
      "0.015625646337866783                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'layers': 'two'}, 'dropout1': 0.3870626376602709, 'dropout2': 0.20031263020060497, 'units1': 182.22144219672182, 'units2': 126.88015379263499}\n",
      "Log-Loss:                                                                          \n",
      "0.016098616644740105                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.18007217979453566, 'layers': 'three', 'units3': 159.67509126388168}, 'dropout1': 0.3688832861996691, 'dropout2': 0.22176160596766006, 'units1': 148.96244924848216, 'units2': 169.1618616610533}\n",
      "Log-Loss:                                                                          \n",
      "0.015742681920528412                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.2808162386473829, 'layers': 'three', 'units3': 206.56969717772114}, 'dropout1': 0.31296208813159004, 'dropout2': 0.1553498450371309, 'units1': 133.69852623478062, 'units2': 190.9112107206659}\n",
      "Log-Loss:                                                                          \n",
      "0.01584029570221901                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'layers': 'two'}, 'dropout1': 0.2894573219639456, 'dropout2': 0.2642945858334917, 'units1': 97.71443775049634, 'units2': 226.8876001587425}\n",
      "Log-Loss:                                                                          \n",
      "0.01674540527164936                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'choice': {'dropout3': 0.3100713984536705, 'layers': 'three', 'units3': 178.93218423246273}, 'dropout1': 0.35588210264544223, 'dropout2': 0.12107021005292007, 'units1': 120.74706281762113, 'units2': 239.35561373701225}\n",
      "Log-Loss:                                                                          \n",
      "0.015581742860376835                                                               \n",
      "100%|██████████| 50/50 [17:08<00:00, 20.57s/trial, best loss: 0.015574329532682896]\n",
      "best:  {'dropout1': 0.37852723044488784, 'dropout2': 0.21729384100246932, 'dropout3': 0.32541189125095554, 'num_layers': 1, 'units1': 134.22875955704404, 'units2': 254.7354081817923, 'units3': 165.51271305572538}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with hyperopt library, seems working and fast.\n",
    "# see https://stackoverflow.com/questions/43533610/how-to-use-hyperopt-for-hyperparameter-optimization-of-keras-deep-learning-netwo\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import sys\n",
    "\n",
    "space = {'choice': hp.choice('num_layers',\n",
    "                    [ {'layers':'two', },\n",
    "                    {'layers':'three',\n",
    "                    'units3': hp.uniform('units3', 64,256), \n",
    "                    'dropout3': hp.uniform('dropout3', .1,.4)}\n",
    "                    ]),\n",
    "\n",
    "            'units1': hp.uniform('units1', 64,256),\n",
    "            'units2': hp.uniform('units2', 64,256),\n",
    "\n",
    "            'dropout1': hp.uniform('dropout1', .1,.4),\n",
    "            'dropout2': hp.uniform('dropout2',  .1,.4),\n",
    "\n",
    "            #'batch_size' : hp.uniform('batch_size', 28,128),\n",
    "\n",
    "            #'epochs' :  30,\n",
    "            #'optimizer': hp.choice('optimizer',['adadelta','adam','rmsprop']),\n",
    "            'activation': 'relu'\n",
    "        }\n",
    "\n",
    "def f_nn(params):   \n",
    "\n",
    "    print ('Params testing: ', params)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['units1'], input_dim = X_pca70_train.shape[1])) \n",
    "    model.add(Activation(params['activation']))\n",
    "    model.add(Dropout(params['dropout1']))\n",
    "\n",
    "    model.add(Dense(params['units2'])) \n",
    "    model.add(Activation(params['activation']))\n",
    "    model.add(Dropout(params['dropout2']))\n",
    "\n",
    "    if params['choice']['layers']== 'three':\n",
    "        model.add(Dense(params['choice']['units3'])) \n",
    "        model.add(Activation(params['activation']))\n",
    "        model.add(Dropout(params['choice']['dropout3']))    \n",
    "\n",
    "    model.add(Dense(206))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=logloss)\n",
    "\n",
    "    model.fit(X_pca70_train, y_train, epochs=30, batch_size=32, verbose = 0)\n",
    "\n",
    "#     pred_auc =model.predict_proba(X_pca70_val, batch_size = 32, verbose = 0)\n",
    "#     acc = roc_auc_score(y_val, pred_auc)\n",
    "#     print('AUC:', acc)\n",
    "#     sys.stdout.flush() \n",
    "#     return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "#     pred = model.predict(X_pca70_val, batch_size = 32, verbose = 0)\n",
    "#     logloss_ = logloss(y_val, pred)\n",
    "    score_logloss = model.evaluate(X_pca70_val, y_val, verbose=0)\n",
    "    print('Log-Loss:', score_logloss[1])\n",
    "    sys.stdout.flush() \n",
    "    return {'loss': score_logloss[1], 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(f_nn, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "print('best: ', best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-detector",
   "metadata": {},
   "source": [
    "**Did learning_rate, decay, label smoothing, batch size optimization manually in cap3_linux.ipynb. Take those optimized parameters, directly combine with above parameters to build final model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "collect-refund",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_218 (Dense)            (None, 134)               9514      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 134)               536       \n",
      "_________________________________________________________________\n",
      "dropout_159 (Dropout)        (None, 134)               0         \n",
      "_________________________________________________________________\n",
      "dense_219 (Dense)            (None, 255)               34425     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 255)               1020      \n",
      "_________________________________________________________________\n",
      "dropout_160 (Dropout)        (None, 255)               0         \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 166)               42496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 166)               664       \n",
      "_________________________________________________________________\n",
      "dropout_161 (Dropout)        (None, 166)               0         \n",
      "_________________________________________________________________\n",
      "dense_221 (Dense)            (None, 206)               34402     \n",
      "=================================================================\n",
      "Total params: 123,057\n",
      "Trainable params: 121,947\n",
      "Non-trainable params: 1,110\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3cb6b981f04e74bed1d5f59da0c35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0batch [00:00, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "477/477 [==============================] - 3s 5ms/step - loss: 0.6200 - logloss: 0.6197 - val_loss: 0.0454 - val_logloss: 0.0436\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.38669, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 2/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0432 - logloss: 0.0412 - val_loss: 0.0243 - val_logloss: 0.0217\n",
      "\n",
      "Epoch 00002: loss improved from 0.38669 to 0.03539, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 3/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0262 - logloss: 0.0235 - val_loss: 0.0227 - val_logloss: 0.0197\n",
      "\n",
      "Epoch 00003: loss improved from 0.03539 to 0.02540, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 4/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0244 - logloss: 0.0214 - val_loss: 0.0220 - val_logloss: 0.0188\n",
      "\n",
      "Epoch 00004: loss improved from 0.02540 to 0.02414, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 5/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0236 - logloss: 0.0204 - val_loss: 0.0216 - val_logloss: 0.0183\n",
      "\n",
      "Epoch 00005: loss improved from 0.02414 to 0.02365, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 6/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0234 - logloss: 0.0201 - val_loss: 0.0212 - val_logloss: 0.0179\n",
      "\n",
      "Epoch 00006: loss improved from 0.02365 to 0.02312, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 7/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0227 - logloss: 0.0194 - val_loss: 0.0210 - val_logloss: 0.0177\n",
      "\n",
      "Epoch 00007: loss improved from 0.02312 to 0.02261, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 8/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0226 - logloss: 0.0193 - val_loss: 0.0207 - val_logloss: 0.0174\n",
      "\n",
      "Epoch 00008: loss improved from 0.02261 to 0.02244, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 9/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0221 - logloss: 0.0188 - val_loss: 0.0205 - val_logloss: 0.0172\n",
      "\n",
      "Epoch 00009: loss improved from 0.02244 to 0.02200, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 10/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0214 - logloss: 0.0182 - val_loss: 0.0204 - val_logloss: 0.0171\n",
      "\n",
      "Epoch 00010: loss improved from 0.02200 to 0.02165, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 11/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0216 - logloss: 0.0183 - val_loss: 0.0202 - val_logloss: 0.0169\n",
      "\n",
      "Epoch 00011: loss improved from 0.02165 to 0.02152, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 12/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0215 - logloss: 0.0182 - val_loss: 0.0201 - val_logloss: 0.0168\n",
      "\n",
      "Epoch 00012: loss improved from 0.02152 to 0.02128, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 13/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0212 - logloss: 0.0179 - val_loss: 0.0201 - val_logloss: 0.0167\n",
      "\n",
      "Epoch 00013: loss improved from 0.02128 to 0.02104, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 14/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0209 - logloss: 0.0177 - val_loss: 0.0198 - val_logloss: 0.0165\n",
      "\n",
      "Epoch 00014: loss improved from 0.02104 to 0.02086, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 15/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0206 - logloss: 0.0174 - val_loss: 0.0198 - val_logloss: 0.0165\n",
      "\n",
      "Epoch 00015: loss improved from 0.02086 to 0.02066, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 16/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0206 - logloss: 0.0174 - val_loss: 0.0197 - val_logloss: 0.0164\n",
      "\n",
      "Epoch 00016: loss improved from 0.02066 to 0.02056, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 17/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0206 - logloss: 0.0174 - val_loss: 0.0196 - val_logloss: 0.0163\n",
      "\n",
      "Epoch 00017: loss improved from 0.02056 to 0.02038, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 18/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0201 - logloss: 0.0169 - val_loss: 0.0195 - val_logloss: 0.0162\n",
      "\n",
      "Epoch 00018: loss improved from 0.02038 to 0.02024, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 19/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0203 - logloss: 0.0171 - val_loss: 0.0194 - val_logloss: 0.0161\n",
      "\n",
      "Epoch 00019: loss improved from 0.02024 to 0.02014, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 20/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0199 - logloss: 0.0167 - val_loss: 0.0194 - val_logloss: 0.0161\n",
      "\n",
      "Epoch 00020: loss improved from 0.02014 to 0.02003, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 21/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0198 - logloss: 0.0166 - val_loss: 0.0194 - val_logloss: 0.0161\n",
      "\n",
      "Epoch 00021: loss improved from 0.02003 to 0.01999, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 22/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0198 - logloss: 0.0166 - val_loss: 0.0194 - val_logloss: 0.0161\n",
      "\n",
      "Epoch 00022: loss improved from 0.01999 to 0.01985, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 23/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0196 - logloss: 0.0165 - val_loss: 0.0193 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00023: loss improved from 0.01985 to 0.01980, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 24/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0197 - logloss: 0.0165 - val_loss: 0.0193 - val_logloss: 0.0160\n",
      "\n",
      "Epoch 00024: loss improved from 0.01980 to 0.01971, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 25/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0195 - logloss: 0.0163 - val_loss: 0.0192 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00025: loss improved from 0.01971 to 0.01960, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 26/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0196 - logloss: 0.0164 - val_loss: 0.0192 - val_logloss: 0.0158\n",
      "\n",
      "Epoch 00026: loss improved from 0.01960 to 0.01955, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 27/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0193 - logloss: 0.0161 - val_loss: 0.0192 - val_logloss: 0.0159\n",
      "\n",
      "Epoch 00027: loss improved from 0.01955 to 0.01951, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 28/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0193 - logloss: 0.0162 - val_loss: 0.0191 - val_logloss: 0.0158\n",
      "\n",
      "Epoch 00028: loss improved from 0.01951 to 0.01942, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 29/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0195 - logloss: 0.0163 - val_loss: 0.0191 - val_logloss: 0.0158\n",
      "\n",
      "Epoch 00029: loss improved from 0.01942 to 0.01930, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 30/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0192 - logloss: 0.0160 - val_loss: 0.0191 - val_logloss: 0.0158\n",
      "\n",
      "Epoch 00030: loss improved from 0.01930 to 0.01930, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 31/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0192 - logloss: 0.0160 - val_loss: 0.0191 - val_logloss: 0.0158\n",
      "\n",
      "Epoch 00031: loss improved from 0.01930 to 0.01925, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 32/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0192 - logloss: 0.0160 - val_loss: 0.0190 - val_logloss: 0.0157\n",
      "\n",
      "Epoch 00032: loss improved from 0.01925 to 0.01922, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 33/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0192 - logloss: 0.0161 - val_loss: 0.0190 - val_logloss: 0.0157\n",
      "\n",
      "Epoch 00033: loss improved from 0.01922 to 0.01913, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 34/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0190 - logloss: 0.0158 - val_loss: 0.0190 - val_logloss: 0.0157\n",
      "\n",
      "Epoch 00034: loss improved from 0.01913 to 0.01910, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 35/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0191 - logloss: 0.0159 - val_loss: 0.0190 - val_logloss: 0.0157\n",
      "\n",
      "Epoch 00035: loss improved from 0.01910 to 0.01908, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 36/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0189 - logloss: 0.0157 - val_loss: 0.0190 - val_logloss: 0.0156\n",
      "\n",
      "Epoch 00036: loss improved from 0.01908 to 0.01905, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 37/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0189 - logloss: 0.0157 - val_loss: 0.0190 - val_logloss: 0.0156\n",
      "\n",
      "Epoch 00037: loss improved from 0.01905 to 0.01897, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 38/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0190 - logloss: 0.0158 - val_loss: 0.0189 - val_logloss: 0.0156\n",
      "\n",
      "Epoch 00038: loss improved from 0.01897 to 0.01895, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 39/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0190 - logloss: 0.0158 - val_loss: 0.0189 - val_logloss: 0.0156\n",
      "\n",
      "Epoch 00039: loss improved from 0.01895 to 0.01890, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 40/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0190 - logloss: 0.0158 - val_loss: 0.0189 - val_logloss: 0.0156\n",
      "\n",
      "Epoch 00040: loss improved from 0.01890 to 0.01888, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 41/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0189 - logloss: 0.0157 - val_loss: 0.0190 - val_logloss: 0.0156\n",
      "\n",
      "Epoch 00041: loss improved from 0.01888 to 0.01885, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 42/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0187 - logloss: 0.0156 - val_loss: 0.0189 - val_logloss: 0.0156\n",
      "\n",
      "Epoch 00042: loss improved from 0.01885 to 0.01881, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 43/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0186 - logloss: 0.0154 - val_loss: 0.0190 - val_logloss: 0.0156\n",
      "\n",
      "Epoch 00043: loss improved from 0.01881 to 0.01879, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 44/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0188 - logloss: 0.0156 - val_loss: 0.0190 - val_logloss: 0.0156\n",
      "\n",
      "Epoch 00044: loss improved from 0.01879 to 0.01871, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 45/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0185 - logloss: 0.0154 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00045: loss improved from 0.01871 to 0.01869, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 46/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0187 - logloss: 0.0155 - val_loss: 0.0189 - val_logloss: 0.0156\n",
      "\n",
      "Epoch 00046: loss improved from 0.01869 to 0.01868, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 47/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0185 - logloss: 0.0153 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.01868\n",
      "Epoch 48/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0185 - logloss: 0.0153 - val_loss: 0.0189 - val_logloss: 0.0156\n",
      "\n",
      "Epoch 00048: loss improved from 0.01868 to 0.01861, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 49/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0185 - logloss: 0.0153 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00049: loss improved from 0.01861 to 0.01859, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 50/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0186 - logloss: 0.0154 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00050: loss improved from 0.01859 to 0.01855, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 51/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0185 - logloss: 0.0153 - val_loss: 0.0188 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.01855\n",
      "Epoch 52/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0185 - logloss: 0.0153 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00052: loss improved from 0.01855 to 0.01851, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 53/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0184 - logloss: 0.0152 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00053: loss improved from 0.01851 to 0.01850, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 54/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0183 - logloss: 0.0151 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00054: loss improved from 0.01850 to 0.01846, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 55/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0184 - logloss: 0.0152 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00055: loss improved from 0.01846 to 0.01845, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 56/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0183 - logloss: 0.0151 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00056: loss improved from 0.01845 to 0.01843, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 57/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0182 - logloss: 0.0150 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00057: loss improved from 0.01843 to 0.01840, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 58/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0184 - logloss: 0.0152 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00058: loss improved from 0.01840 to 0.01838, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 59/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0182 - logloss: 0.0150 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00059: loss improved from 0.01838 to 0.01835, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 60/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0182 - logloss: 0.0150 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.01835\n",
      "Epoch 61/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0182 - logloss: 0.0150 - val_loss: 0.0188 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.01835\n",
      "Epoch 62/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0182 - logloss: 0.0149 - val_loss: 0.0188 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00062: loss improved from 0.01835 to 0.01831, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 63/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0181 - logloss: 0.0149 - val_loss: 0.0188 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00063: loss improved from 0.01831 to 0.01827, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 64/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0182 - logloss: 0.0149 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.01827\n",
      "Epoch 65/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0180 - logloss: 0.0148 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00065: loss improved from 0.01827 to 0.01820, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 66/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0181 - logloss: 0.0149 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.01820\n",
      "Epoch 67/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0182 - logloss: 0.0149 - val_loss: 0.0188 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00067: loss improved from 0.01820 to 0.01820, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 68/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0182 - logloss: 0.0150 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00068: loss improved from 0.01820 to 0.01818, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 69/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0180 - logloss: 0.0148 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00069: loss improved from 0.01818 to 0.01817, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 70/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0182 - logloss: 0.0150 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.01817\n",
      "Epoch 71/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0182 - logloss: 0.0150 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00071: loss improved from 0.01817 to 0.01815, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 72/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0180 - logloss: 0.0148 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00072: loss improved from 0.01815 to 0.01810, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 73/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0180 - logloss: 0.0148 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.01810\n",
      "Epoch 74/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0181 - logloss: 0.0149 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00074: loss improved from 0.01810 to 0.01808, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 75/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0179 - logloss: 0.0147 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.01808\n",
      "Epoch 76/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0178 - logloss: 0.0146 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00076: loss improved from 0.01808 to 0.01804, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 77/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0180 - logloss: 0.0148 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.01804\n",
      "Epoch 78/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0180 - logloss: 0.0147 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.01804\n",
      "Epoch 79/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0180 - logloss: 0.0148 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00079: loss improved from 0.01804 to 0.01798, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 80/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0180 - logloss: 0.0148 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.01798\n",
      "Epoch 81/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0180 - logloss: 0.0148 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.01798\n",
      "Epoch 82/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0145 - val_loss: 0.0188 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00082: loss improved from 0.01798 to 0.01793, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 83/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0181 - logloss: 0.0149 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.01793\n",
      "Epoch 84/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0179 - logloss: 0.0147 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.01793\n",
      "Epoch 85/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0178 - logloss: 0.0146 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.01793\n",
      "Epoch 86/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0179 - logloss: 0.0147 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.01793\n",
      "Epoch 87/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0179 - logloss: 0.0147 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00087: loss improved from 0.01793 to 0.01790, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 88/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0179 - logloss: 0.0147 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00088: loss improved from 0.01790 to 0.01789, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 89/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0179 - logloss: 0.0147 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.01789\n",
      "Epoch 90/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0145 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00090: loss did not improve from 0.01789\n",
      "Epoch 91/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0179 - logloss: 0.0147 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00091: loss improved from 0.01789 to 0.01784, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 92/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0178 - logloss: 0.0145 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.01784\n",
      "Epoch 93/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0178 - logloss: 0.0146 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00093: loss improved from 0.01784 to 0.01783, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 94/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0145 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.01783\n",
      "Epoch 95/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0145 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.01783\n",
      "Epoch 96/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0176 - logloss: 0.0144 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00096: loss improved from 0.01783 to 0.01781, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 97/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0178 - logloss: 0.0145 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00097: loss improved from 0.01781 to 0.01781, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 98/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0178 - logloss: 0.0145 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00098: loss improved from 0.01781 to 0.01777, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 99/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0178 - logloss: 0.0146 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.01777\n",
      "Epoch 100/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0176 - logloss: 0.0144 - val_loss: 0.0188 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.01777\n",
      "Epoch 101/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0144 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00101: loss did not improve from 0.01777\n",
      "Epoch 102/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0144 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00102: loss did not improve from 0.01777\n",
      "Epoch 103/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0145 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00103: loss improved from 0.01777 to 0.01776, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 104/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0145 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00104: loss improved from 0.01776 to 0.01774, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 105/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0178 - logloss: 0.0145 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00105: loss did not improve from 0.01774\n",
      "Epoch 106/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0145 - val_loss: 0.0188 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00106: loss improved from 0.01774 to 0.01770, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 107/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0144 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00107: loss did not improve from 0.01770\n",
      "Epoch 108/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0144 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00108: loss did not improve from 0.01770\n",
      "Epoch 109/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0145 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00109: loss improved from 0.01770 to 0.01768, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 110/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00110: loss improved from 0.01768 to 0.01766, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 111/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00111: loss did not improve from 0.01766\n",
      "Epoch 112/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00112: loss improved from 0.01766 to 0.01765, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 113/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0144 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00113: loss did not improve from 0.01765\n",
      "Epoch 114/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00114: loss improved from 0.01765 to 0.01765, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 115/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00115: loss did not improve from 0.01765\n",
      "Epoch 116/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0142 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00116: loss improved from 0.01765 to 0.01764, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 117/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00117: loss did not improve from 0.01764\n",
      "Epoch 118/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0144 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00118: loss improved from 0.01764 to 0.01763, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 119/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0144 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00119: loss did not improve from 0.01763\n",
      "Epoch 120/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0142 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00120: loss did not improve from 0.01763\n",
      "Epoch 121/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0177 - logloss: 0.0145 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00121: loss improved from 0.01763 to 0.01757, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 122/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00122: loss did not improve from 0.01757\n",
      "Epoch 123/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00123: loss did not improve from 0.01757\n",
      "Epoch 124/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00124: loss did not improve from 0.01757\n",
      "Epoch 125/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0142 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00125: loss did not improve from 0.01757\n",
      "Epoch 126/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00126: loss improved from 0.01757 to 0.01755, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 127/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0174 - logloss: 0.0141 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00127: loss improved from 0.01755 to 0.01750, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 128/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00128: loss improved from 0.01750 to 0.01747, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 129/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0176 - logloss: 0.0144 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00129: loss did not improve from 0.01747\n",
      "Epoch 130/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0142 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00130: loss did not improve from 0.01747\n",
      "Epoch 131/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00131: loss did not improve from 0.01747\n",
      "Epoch 132/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0142 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00132: loss did not improve from 0.01747\n",
      "Epoch 133/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0174 - logloss: 0.0142 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00133: loss did not improve from 0.01747\n",
      "Epoch 134/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0173 - logloss: 0.0141 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00134: loss improved from 0.01747 to 0.01746, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 135/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0142 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00135: loss did not improve from 0.01746\n",
      "Epoch 136/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0142 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00136: loss did not improve from 0.01746\n",
      "Epoch 137/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0173 - logloss: 0.0140 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00137: loss did not improve from 0.01746\n",
      "Epoch 138/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0174 - logloss: 0.0141 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00138: loss did not improve from 0.01746\n",
      "Epoch 139/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00139: loss improved from 0.01746 to 0.01745, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 140/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0139 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00140: loss improved from 0.01745 to 0.01740, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 141/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0175 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00141: loss did not improve from 0.01740\n",
      "Epoch 142/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0174 - logloss: 0.0141 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00142: loss did not improve from 0.01740\n",
      "Epoch 143/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00143: loss did not improve from 0.01740\n",
      "Epoch 144/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0173 - logloss: 0.0140 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00144: loss did not improve from 0.01740\n",
      "Epoch 145/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0174 - logloss: 0.0141 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00145: loss did not improve from 0.01740\n",
      "Epoch 146/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0173 - logloss: 0.0140 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00146: loss improved from 0.01740 to 0.01740, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 147/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00147: loss did not improve from 0.01740\n",
      "Epoch 148/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0174 - logloss: 0.0142 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00148: loss improved from 0.01740 to 0.01739, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 149/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0140 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00149: loss improved from 0.01739 to 0.01732, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 150/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0173 - logloss: 0.0140 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00150: loss did not improve from 0.01732\n",
      "Epoch 151/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0173 - logloss: 0.0140 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00151: loss did not improve from 0.01732\n",
      "Epoch 152/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0173 - logloss: 0.0140 - val_loss: 0.0190 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00152: loss did not improve from 0.01732\n",
      "Epoch 153/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0173 - logloss: 0.0140 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00153: loss did not improve from 0.01732\n",
      "Epoch 154/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0139 - val_loss: 0.0190 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00154: loss did not improve from 0.01732\n",
      "Epoch 155/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0139 - val_loss: 0.0189 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00155: loss did not improve from 0.01732\n",
      "Epoch 156/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0173 - logloss: 0.0141 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00156: loss improved from 0.01732 to 0.01732, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 157/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00157: loss improved from 0.01732 to 0.01730, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 158/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00158: loss did not improve from 0.01730\n",
      "Epoch 159/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0173 - logloss: 0.0140 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00159: loss did not improve from 0.01730\n",
      "Epoch 160/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0174 - logloss: 0.0141 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00160: loss did not improve from 0.01730\n",
      "Epoch 161/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00161: loss improved from 0.01730 to 0.01729, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 162/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00162: loss improved from 0.01729 to 0.01720, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 163/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00163: loss did not improve from 0.01720\n",
      "Epoch 164/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0139 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00164: loss did not improve from 0.01720\n",
      "Epoch 165/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0139 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00165: loss did not improve from 0.01720\n",
      "Epoch 166/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00166: loss did not improve from 0.01720\n",
      "Epoch 167/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0170 - logloss: 0.0137 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00167: loss did not improve from 0.01720\n",
      "Epoch 168/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00168: loss improved from 0.01720 to 0.01719, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 169/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00169: loss did not improve from 0.01719\n",
      "Epoch 170/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0173 - logloss: 0.0141 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00170: loss did not improve from 0.01719\n",
      "Epoch 171/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0139 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00171: loss did not improve from 0.01719\n",
      "Epoch 172/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0139 - val_loss: 0.0190 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00172: loss did not improve from 0.01719\n",
      "Epoch 173/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0139 - val_loss: 0.0190 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00173: loss did not improve from 0.01719\n",
      "Epoch 174/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0169 - logloss: 0.0136 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00174: loss improved from 0.01719 to 0.01719, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 175/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0140 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00175: loss did not improve from 0.01719\n",
      "Epoch 176/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0170 - logloss: 0.0137 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00176: loss did not improve from 0.01719\n",
      "Epoch 177/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00177: loss improved from 0.01719 to 0.01718, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 178/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0139 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00178: loss did not improve from 0.01718\n",
      "Epoch 179/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00179: loss did not improve from 0.01718\n",
      "Epoch 180/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0139 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00180: loss did not improve from 0.01718\n",
      "Epoch 181/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0169 - logloss: 0.0137 - val_loss: 0.0190 - val_logloss: 0.0154\n",
      "\n",
      "Epoch 00181: loss improved from 0.01718 to 0.01717, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 182/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0139 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00182: loss did not improve from 0.01717\n",
      "Epoch 183/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0139 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00183: loss did not improve from 0.01717\n",
      "Epoch 184/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00184: loss did not improve from 0.01717\n",
      "Epoch 185/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0169 - logloss: 0.0136 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00185: loss improved from 0.01717 to 0.01716, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 186/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00186: loss did not improve from 0.01716\n",
      "Epoch 187/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0173 - logloss: 0.0140 - val_loss: 0.0191 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00187: loss improved from 0.01716 to 0.01716, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 188/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0170 - logloss: 0.0137 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00188: loss improved from 0.01716 to 0.01713, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 189/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0173 - logloss: 0.0140 - val_loss: 0.0191 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00189: loss did not improve from 0.01713\n",
      "Epoch 190/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00190: loss improved from 0.01713 to 0.01711, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 191/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0169 - logloss: 0.0136 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00191: loss improved from 0.01711 to 0.01711, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 192/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0169 - logloss: 0.0136 - val_loss: 0.0191 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00192: loss improved from 0.01711 to 0.01706, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 193/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00193: loss did not improve from 0.01706\n",
      "Epoch 194/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00194: loss did not improve from 0.01706\n",
      "Epoch 195/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0191 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00195: loss did not improve from 0.01706\n",
      "Epoch 196/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0170 - logloss: 0.0137 - val_loss: 0.0191 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00196: loss did not improve from 0.01706\n",
      "Epoch 197/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00197: loss did not improve from 0.01706\n",
      "Epoch 198/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00198: loss improved from 0.01706 to 0.01704, saving model to models/mlp_final\n",
      "INFO:tensorflow:Assets written to: models/mlp_final/assets\n",
      "Epoch 199/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0170 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00199: loss did not improve from 0.01704\n",
      "Epoch 200/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0169 - logloss: 0.0137 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00200: loss did not improve from 0.01704\n",
      "Epoch 201/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00201: loss did not improve from 0.01704\n",
      "Epoch 202/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0170 - logloss: 0.0137 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00202: loss did not improve from 0.01704\n",
      "Epoch 203/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0191 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00203: loss did not improve from 0.01704\n",
      "Epoch 204/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0169 - logloss: 0.0136 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00204: loss did not improve from 0.01704\n",
      "Epoch 205/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0139 - val_loss: 0.0191 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00205: loss did not improve from 0.01704\n",
      "Epoch 206/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0172 - logloss: 0.0139 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00206: loss did not improve from 0.01704\n",
      "Epoch 207/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0171 - logloss: 0.0138 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00207: loss did not improve from 0.01704\n",
      "Epoch 208/500\n",
      "477/477 [==============================] - 2s 5ms/step - loss: 0.0170 - logloss: 0.0137 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "\n",
      "Epoch 00208: loss did not improve from 0.01704\n",
      "Validation log-loss:  0.015460934489965439\n",
      "INFO:tensorflow:Assets written to: models/mlp_final_complete/assets\n",
      "Total running time: 10.451560493310293 minutes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To record running time\n",
    "start = time.time()\n",
    "\n",
    "# build final model with optimized hyperparameters, print model summary\n",
    "def mlp_model(n_cols):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(134, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.379))\n",
    "    model.add(Dense(255, activation='relu' ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.217))\n",
    "    model.add(Dense(166, activation='relu' ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.325))\n",
    "    model.add(Dense(206, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=0.0005, decay=0.001/200), loss=BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n",
    "\n",
    "    return model \n",
    "\n",
    "n_cols = X_pca70_train.shape[1]\n",
    "mlp_final = mlp_model(n_cols)\n",
    "mlp_final.summary()\n",
    "\n",
    "# setup callbacks for fitting\n",
    "earlystp_mlp_final = EarlyStopping(monitor='loss', patience=10)\n",
    "filepath = 'models/mlp_final'\n",
    "checkpoint_mlp_final = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# fit the model\n",
    "history_mlp_final = mlp_final.fit(\n",
    "    X_pca70_train, y_train, batch_size=32,\n",
    "    validation_data=(X_pca70_val, y_val),\n",
    "    epochs=500,\n",
    "    callbacks=[checkpoint_mlp_final, earlystp_mlp_final, TqdmCallback()])\n",
    "\n",
    "# evaluate model with validation data\n",
    "score = mlp_final.evaluate(X_pca70_val, y_val, verbose=0)\n",
    "print('Validation log-loss: ', score[1])\n",
    "\n",
    "# save the model\n",
    "mlp_final.save('models/mlp_final_complete')\n",
    "\n",
    "# plot training history of the model\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "[ax.plot(history_mlp_final.history[i], label=i) for i in ['logloss', 'val_logloss']] #history_alexnet.history.keys()]\n",
    "ax.set_xlabel('Epochs', fontsize=15)\n",
    "ax.set_ylabel('Log-Loss', fontsize=15)\n",
    "ax.set_title('Final MLP Model with PCA (n_components=70)', fontsize=20)\n",
    "plt.legend(fontsize=13, loc='best')\n",
    "plt.savefig('images/mlp_final.jpg')\n",
    "\n",
    "# calculate the running time\n",
    "end = time.time()\n",
    "print(\"Total running time:\", (end - start)/60, 'minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "latter-development",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4763, 206),\n",
       " array([4.26311686e-04, 3.15509387e-04, 6.81524049e-04, 5.53430617e-03,\n",
       "        1.56197082e-02, 7.14969356e-03, 4.46298718e-03, 2.81365751e-03,\n",
       "        2.82501249e-04, 1.67086273e-02, 2.72273663e-02, 1.21686971e-04,\n",
       "        1.65818070e-04, 1.22064550e-04, 1.60292734e-03, 2.32351391e-04,\n",
       "        2.15103826e-03, 1.09791756e-03, 8.11889675e-03, 3.15964245e-03,\n",
       "        8.20008689e-04, 1.70584093e-03, 3.87792446e-04, 1.06420461e-03,\n",
       "        1.18138990e-03, 3.01843800e-04, 5.62351721e-04, 3.68266512e-04,\n",
       "        7.24778324e-03, 2.83170422e-03, 5.22994727e-04, 1.28603401e-03,\n",
       "        4.98053036e-04, 2.46029405e-04, 5.00624243e-04, 2.47914344e-04,\n",
       "        4.19031456e-03, 1.95220549e-04, 2.81328015e-04, 4.37082723e-04,\n",
       "        2.32636323e-03, 2.49067508e-03, 1.91137916e-03, 9.85627156e-03,\n",
       "        1.27282704e-03, 2.22069095e-03, 1.97914356e-04, 8.20291578e-04,\n",
       "        2.11449820e-04, 3.45877558e-03, 1.01739739e-03, 1.41989114e-03,\n",
       "        3.47551832e-04, 1.06922619e-03, 1.48599707e-02, 2.16580438e-03,\n",
       "        4.08518768e-04, 2.44400394e-03, 1.13578478e-03, 2.57202494e-03,\n",
       "        2.37265354e-04, 4.25096042e-03, 4.31559514e-04, 2.15169857e-04,\n",
       "        7.14799203e-03, 3.32852302e-04, 1.57268380e-03, 7.11263670e-03,\n",
       "        4.95181419e-03, 3.68413981e-04, 4.54110064e-04, 3.90938297e-02,\n",
       "        1.12657603e-02, 1.97948772e-03, 9.17314785e-04, 7.82955962e-04,\n",
       "        4.74857487e-04, 6.51226472e-03, 7.74733210e-03, 7.50691444e-03,\n",
       "        2.96584185e-04, 8.18950124e-04, 3.45590728e-04, 1.49635514e-02,\n",
       "        2.01592548e-03, 2.96327937e-03, 4.15405462e-04, 3.43995052e-04,\n",
       "        5.53165810e-05, 5.61926790e-05, 7.84639997e-05, 9.42136568e-04,\n",
       "        3.61621758e-04, 7.88671803e-03, 4.64427890e-03, 4.85223311e-04,\n",
       "        1.16883224e-04, 1.04904128e-03, 5.56183793e-03, 1.97675787e-02,\n",
       "        1.18610088e-03, 7.38512957e-04, 1.79956690e-03, 5.21059614e-04,\n",
       "        1.48845033e-03, 1.56712383e-02, 4.60250158e-04, 1.09684339e-03,\n",
       "        2.04337155e-03, 8.74640187e-04, 1.54063833e-04, 2.87093455e-04,\n",
       "        6.08287810e-04, 2.46396568e-03, 1.97543483e-03, 2.63870344e-03,\n",
       "        2.53055757e-03, 3.89473187e-03, 2.87891075e-04, 3.37862621e-05,\n",
       "        4.40324104e-04, 3.19127226e-04, 7.43148499e-04, 8.18216940e-04,\n",
       "        1.86602888e-03, 5.39931061e-04, 1.69155232e-04, 2.49117322e-04,\n",
       "        8.92617740e-03, 1.90624269e-03, 3.82650906e-04, 8.88648629e-03,\n",
       "        1.62182914e-04, 1.67117186e-03, 9.78338160e-03, 3.40895203e-04,\n",
       "        3.18065821e-03, 4.89308091e-04, 1.27410237e-03, 6.51032722e-04,\n",
       "        6.22932508e-04, 4.77103807e-04, 7.20186799e-04, 2.23728875e-03,\n",
       "        2.71382625e-03, 1.82068057e-03, 6.84198676e-05, 5.69800322e-04,\n",
       "        2.29417667e-04, 3.25801520e-05, 1.37133594e-03, 6.93376083e-03,\n",
       "        1.35682290e-03, 1.63452572e-03, 2.02842435e-04, 8.81110260e-04,\n",
       "        5.36730550e-02, 2.32741702e-03, 1.61335478e-03, 7.26172875e-04,\n",
       "        2.29882731e-04, 1.73389772e-03, 5.20547526e-03, 1.98174952e-04,\n",
       "        1.02845288e-03, 2.46220152e-04, 3.61594604e-03, 5.11946506e-04,\n",
       "        1.25348661e-03, 5.67358657e-05, 1.16336590e-03, 1.14075409e-03,\n",
       "        2.95074133e-04, 5.68424992e-04, 5.48907032e-04, 2.33728089e-03,\n",
       "        5.18811084e-02, 1.08568175e-02, 2.09927629e-03, 4.10786876e-03,\n",
       "        2.80787190e-03, 5.94386482e-04, 2.00286675e-02, 1.23640720e-03,\n",
       "        5.54281229e-04, 1.51267624e-03, 2.01863266e-04, 1.15901616e-03,\n",
       "        1.08319844e-04, 1.96051755e-04, 3.26834997e-04, 9.34126438e-04,\n",
       "        7.51597167e-04, 4.51864069e-03, 2.88362498e-04, 5.95842779e-04,\n",
       "        3.28536960e-04, 4.72365908e-04, 6.69469417e-04, 1.37877220e-03,\n",
       "        5.35643136e-04, 4.77843831e-04, 1.22127763e-04, 9.29762318e-04,\n",
       "        8.96990125e-04, 9.11619107e-04], dtype=float32))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = mlp_final.predict(X_pca70_test)\n",
    "y_test_pred.shape, y_test_pred[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "front-habitat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015728436410427094"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score = mlp_final.evaluate(X_pca70_test, y_test, verbose=0)[1]\n",
    "test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-asian",
   "metadata": {},
   "source": [
    "**Final logloss on holdout data is 0.0157.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-train",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "respected-playback",
   "metadata": {},
   "source": [
    "Play around more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "loaded-fountain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015394747257232666"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(history_mlp_final.history['val_logloss']).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "complicated-ethernet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015460934489965439"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_final.evaluate(X_pca70_val, y_val, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-punishment",
   "metadata": {},
   "source": [
    "looks like the best model is somewhere in the 208 epochs. So load it from callbacks, and re-calculate the best score for validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "centered-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With custom object, have to do following to load model, otherwise, will always throw errors here or later\n",
    "best_model = load_model('models/mlp_final', compile=False, custom_objects={\"logloss\": logloss})\n",
    "best_model.compile(optimizer=Adam(lr=0.001, decay=0.001/200), loss=BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "functional-clothing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01547936163842678"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_pca70_val, y_val, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-sigma",
   "metadata": {},
   "source": [
    "Seems even I loaded the best model (because I saved only best model in callbacks), still not gave the lowest logloss? Problem comes from the reload??? Try to fit more and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "outdoor-premiere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "477/477 [==============================] - 3s 4ms/step - loss: 0.0175 - logloss: 0.0142 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "Epoch 2/10\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "Epoch 3/10\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0190 - val_logloss: 0.0156\n",
      "Epoch 4/10\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "Epoch 5/10\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "Epoch 6/10\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "Epoch 7/10\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0176 - logloss: 0.0144 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "Epoch 8/10\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0176 - logloss: 0.0144 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "Epoch 9/10\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0190 - val_logloss: 0.0155\n",
      "Epoch 10/10\n",
      "477/477 [==============================] - 2s 4ms/step - loss: 0.0176 - logloss: 0.0143 - val_loss: 0.0190 - val_logloss: 0.0155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc56007ffd0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(\n",
    "    X_pca70_train, y_train, batch_size=32,\n",
    "    validation_data=(X_pca70_val, y_val),\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "chicken-marijuana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015506485477089882"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_pca70_val, y_val, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-chase",
   "metadata": {},
   "source": [
    "Even worse!!! So stop here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-merchant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-links",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "limited-joyce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params testing:                                       \n",
      "{'activation': 'relu', 'batch_size': 256, 'choice': {'dropout3': 0.20096682915867073, 'layers': 'three', 'units3': 65.1701393644116}, 'dropout1': 0.4490989767251067, 'dropout2': 0.10005130704392862, 'learn_rate': 0.00024647911577451115, 'units1': 92.01021022429563, 'units2': 125.58026541235671}\n",
      "Log-Loss:                                             \n",
      "0.019952604547142982                                  \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.22947026236945753, 'layers': 'three', 'units3': 209.8354751804819}, 'dropout1': 0.34135003794646634, 'dropout2': 0.44715722668535285, 'learn_rate': 0.00016628448410201577, 'units1': 235.23851448418333, 'units2': 140.6480292312059}\n",
      "Log-Loss:                                                                         \n",
      "0.017096811905503273                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 64, 'choice': {'layers': 'two'}, 'dropout1': 0.39066937653040157, 'dropout2': 0.26861341499041247, 'learn_rate': 0.00030272755002634503, 'units1': 222.86930731723416, 'units2': 162.10523456811293}\n",
      "Log-Loss:                                                                         \n",
      "0.01604999229311943                                                               \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'layers': 'two'}, 'dropout1': 0.33591879187062756, 'dropout2': 0.2095796179604218, 'learn_rate': 0.0004547429112548889, 'units1': 212.92250384492817, 'units2': 187.8211565596469}\n",
      "Log-Loss:                                                                        \n",
      "0.01604549214243889                                                              \n",
      "Params testing:                                                                  \n",
      "{'activation': 'relu', 'batch_size': 256, 'choice': {'dropout3': 0.33646949129926296, 'layers': 'three', 'units3': 158.42676947233988}, 'dropout1': 0.3497231619571793, 'dropout2': 0.1770920841482937, 'learn_rate': 0.00026995663473843, 'units1': 121.64357319327294, 'units2': 107.0066535257811}\n",
      "Log-Loss:                                                                        \n",
      "0.018831513822078705                                                             \n",
      "Params testing:                                                                  \n",
      "{'activation': 'relu', 'batch_size': 256, 'choice': {'dropout3': 0.21465662110023673, 'layers': 'three', 'units3': 151.487437449889}, 'dropout1': 0.23634241472915726, 'dropout2': 0.396963066118458, 'learn_rate': 0.0009445162781471639, 'units1': 86.79418532407615, 'units2': 219.5612371694958}\n",
      "Log-Loss:                                                                        \n",
      "0.01697489246726036                                                              \n",
      "Params testing:                                                                  \n",
      "{'activation': 'relu', 'batch_size': 128, 'choice': {'layers': 'two'}, 'dropout1': 0.29999867741383324, 'dropout2': 0.26092501121320255, 'learn_rate': 0.0003013986578299167, 'units1': 127.92631126462544, 'units2': 248.33747650850694}\n",
      "Log-Loss:                                                                        \n",
      "0.01646680198609829                                                              \n",
      "Params testing:                                                                  \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'layers': 'two'}, 'dropout1': 0.41696048644189976, 'dropout2': 0.30336269976154706, 'learn_rate': 0.0009180692296009668, 'units1': 185.53667009704463, 'units2': 166.9717597883164}\n",
      "Log-Loss:                                                                        \n",
      "0.016098925843834877                                                             \n",
      "Params testing:                                                                  \n",
      "{'activation': 'relu', 'batch_size': 256, 'choice': {'layers': 'two'}, 'dropout1': 0.1625448377287825, 'dropout2': 0.1964171872624171, 'learn_rate': 0.0004505575829073612, 'units1': 205.96651486195373, 'units2': 254.66437472308635}\n",
      "Log-Loss:                                                                        \n",
      "0.01638166233897209                                                              \n",
      "Params testing:                                                                  \n",
      "{'activation': 'relu', 'batch_size': 64, 'choice': {'dropout3': 0.44101743219604095, 'layers': 'three', 'units3': 90.62543820969586}, 'dropout1': 0.3292338001726479, 'dropout2': 0.22067820698303234, 'learn_rate': 0.0007401429683665816, 'units1': 242.17772068723764, 'units2': 226.14412233609107}\n",
      "Log-Loss:                                                                        \n",
      "0.015921572223305702                                                             \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.4060767777876956, 'layers': 'three', 'units3': 181.39000041561508}, 'dropout1': 0.37367085066396, 'dropout2': 0.3535145319964694, 'learn_rate': 0.0007891386380696378, 'units1': 177.7156978366146, 'units2': 245.7779761065957}\n",
      "Log-Loss:                                                                          \n",
      "0.015734262764453888                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.17842473322139418, 'layers': 'three', 'units3': 64.74176892173588}, 'dropout1': 0.3912764153095335, 'dropout2': 0.45404647015108457, 'learn_rate': 0.0006544967633150331, 'units1': 106.7093475511058, 'units2': 100.28681846444101}\n",
      "Log-Loss:                                                                          \n",
      "0.01596381701529026                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 128, 'choice': {'dropout3': 0.41987692414404965, 'layers': 'three', 'units3': 133.36708229407043}, 'dropout1': 0.33152690409623453, 'dropout2': 0.2742169322549689, 'learn_rate': 0.0007153462217395618, 'units1': 236.2268005705532, 'units2': 71.13158249130001}\n",
      "Log-Loss:                                                                          \n",
      "0.016366666182875633                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 256, 'choice': {'layers': 'two'}, 'dropout1': 0.49384628627130955, 'dropout2': 0.11673446450944427, 'learn_rate': 0.0002521261239433303, 'units1': 118.47834413772266, 'units2': 90.64347194746438}\n",
      "Log-Loss:                                                                          \n",
      "0.018361276015639305                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 256, 'choice': {'layers': 'two'}, 'dropout1': 0.3302778405278213, 'dropout2': 0.15777981149789966, 'learn_rate': 0.0006364333998604056, 'units1': 226.42201252370737, 'units2': 200.84937126764305}\n",
      "Log-Loss:                                                                          \n",
      "0.01626955345273018                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 256, 'choice': {'layers': 'two'}, 'dropout1': 0.10699018016354872, 'dropout2': 0.3084928345148562, 'learn_rate': 0.0001738786520873415, 'units1': 143.24646807062223, 'units2': 211.13483542904362}\n",
      "Log-Loss:                                                                          \n",
      "0.017910940572619438                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'layers': 'two'}, 'dropout1': 0.41080318597719767, 'dropout2': 0.4791157163663956, 'learn_rate': 0.0006766423137917905, 'units1': 77.42377053217153, 'units2': 152.96928813971533}\n",
      "Log-Loss:                                                                          \n",
      "0.01581360213458538                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 256, 'choice': {'dropout3': 0.3523877372833646, 'layers': 'three', 'units3': 65.49664619538854}, 'dropout1': 0.11380949225986048, 'dropout2': 0.21416040161299837, 'learn_rate': 0.0006544379554220443, 'units1': 222.99053090365712, 'units2': 166.3492909304108}\n",
      "Log-Loss:                                                                          \n",
      "0.016551924869418144                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'layers': 'two'}, 'dropout1': 0.20804098688329975, 'dropout2': 0.42593545555064016, 'learn_rate': 0.0007515698824731345, 'units1': 91.98217478375079, 'units2': 164.05967255420106}\n",
      "Log-Loss:                                                                          \n",
      "0.01568192057311535                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.426026768598444, 'layers': 'three', 'units3': 213.4675286979244}, 'dropout1': 0.2035078159408649, 'dropout2': 0.19167193738358201, 'learn_rate': 0.0003991023861528828, 'units1': 251.9538258084739, 'units2': 155.64401800320948}\n",
      "Log-Loss:                                                                         \n",
      "0.01566234417259693                                                               \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'layers': 'two'}, 'dropout1': 0.21186756463300357, 'dropout2': 0.38754703193230333, 'learn_rate': 0.0005255244277363732, 'units1': 64.25356479252676, 'units2': 130.3068033570999}\n",
      "Log-Loss:                                                                         \n",
      "0.015848247334361076                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.4990085914517065, 'layers': 'three', 'units3': 250.15063666124806}, 'dropout1': 0.22950219737422323, 'dropout2': 0.13581250920602864, 'learn_rate': 0.000850591466274188, 'units1': 154.3071265903891, 'units2': 182.80334057657763}\n",
      "Log-Loss:                                                                         \n",
      "0.01581951044499874                                                               \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'layers': 'two'}, 'dropout1': 0.17290441192048153, 'dropout2': 0.3476776041181108, 'learn_rate': 0.0003859371072061143, 'units1': 178.3812638099635, 'units2': 185.7185301399943}\n",
      "Log-Loss:                                                                         \n",
      "0.015765199437737465                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 128, 'choice': {'dropout3': 0.1288038520448263, 'layers': 'three', 'units3': 255.68090274737412}, 'dropout1': 0.2824508429127381, 'dropout2': 0.4192024799213737, 'learn_rate': 0.0005559857137581982, 'units1': 68.98691333620937, 'units2': 144.60498285520853}\n",
      "Log-Loss:                                                                         \n",
      "0.016926098614931107                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 64, 'choice': {'dropout3': 0.4954095835358482, 'layers': 'three', 'units3': 221.69144951990268}, 'dropout1': 0.15557030926264365, 'dropout2': 0.3465833527867318, 'learn_rate': 0.00010018266204098246, 'units1': 147.3319243402545, 'units2': 123.9385230172262}\n",
      "Log-Loss:                                                                         \n",
      "0.017680959776043892                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'layers': 'two'}, 'dropout1': 0.26844273094766136, 'dropout2': 0.497994140362337, 'learn_rate': 0.000997930678086761, 'units1': 193.39254078060955, 'units2': 174.55205437362915}\n",
      "Log-Loss:                                                                         \n",
      "0.015741761773824692                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.29481832027986177, 'layers': 'three', 'units3': 113.37364356889253}, 'dropout1': 0.19174829196938512, 'dropout2': 0.23895580859049168, 'learn_rate': 0.0005670572900887433, 'units1': 168.204561857666, 'units2': 64.54275985433122}\n",
      "Log-Loss:                                                                         \n",
      "0.01575305685400963                                                               \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 128, 'choice': {'layers': 'two'}, 'dropout1': 0.25135087251111954, 'dropout2': 0.15578655929288499, 'learn_rate': 0.0003950688951032238, 'units1': 99.54626936396218, 'units2': 195.2818977087972}\n",
      "Log-Loss:                                                                         \n",
      "0.0163738951086998                                                                \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.3004202147938686, 'layers': 'three', 'units3': 193.99604379566273}, 'dropout1': 0.13654595423612823, 'dropout2': 0.3297351024156383, 'learn_rate': 0.0008298623900459335, 'units1': 138.28891432064415, 'units2': 122.6113978773738}\n",
      "Log-Loss:                                                                         \n",
      "0.015812110155820847                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 64, 'choice': {'layers': 'two'}, 'dropout1': 0.20453361716614896, 'dropout2': 0.38558118869993874, 'learn_rate': 0.0004930084793030235, 'units1': 251.5795601835572, 'units2': 147.45132131080223}\n",
      "Log-Loss:                                                                         \n",
      "0.015794804319739342                                                              \n",
      "Params testing:                                                                   \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.4617516322737893, 'layers': 'three', 'units3': 233.0528818920695}, 'dropout1': 0.13163410928243874, 'dropout2': 0.10561668815571484, 'learn_rate': 0.0003926668172025186, 'units1': 109.51059889064979, 'units2': 136.13374296124888}\n",
      "Log-Loss:                                                                         \n",
      "0.015660621225833893                                                              \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.46238258508140373, 'layers': 'three', 'units3': 232.46924689797885}, 'dropout1': 0.13528554242185825, 'dropout2': 0.10433104587698946, 'learn_rate': 0.0003663800315804992, 'units1': 107.38067114070712, 'units2': 79.70788606316054}\n",
      "Log-Loss:                                                                          \n",
      "0.015814684331417084                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 64, 'choice': {'dropout3': 0.3834112931894067, 'layers': 'three', 'units3': 186.11104780858778}, 'dropout1': 0.10641279241765156, 'dropout2': 0.13771410407480728, 'learn_rate': 0.00019703728575702541, 'units1': 160.10875570215242, 'units2': 112.17578408394202}\n",
      "Log-Loss:                                                                          \n",
      "0.01632198505103588                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.47304589924514434, 'layers': 'three', 'units3': 239.0842370702092}, 'dropout1': 0.18214301553556594, 'dropout2': 0.18646823963172657, 'learn_rate': 0.0003407170770568572, 'units1': 253.21974032033722, 'units2': 137.90255327945272}\n",
      "Log-Loss:                                                                          \n",
      "0.01577194593846798                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.3759098896578125, 'layers': 'three', 'units3': 207.98145503362755}, 'dropout1': 0.13955861014346665, 'dropout2': 0.10049270724796788, 'learn_rate': 0.0001147004801692122, 'units1': 196.8744763064888, 'units2': 115.78779583145023}\n",
      "Log-Loss:                                                                          \n",
      "0.016193490475416183                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 128, 'choice': {'dropout3': 0.44486554210456436, 'layers': 'three', 'units3': 219.99827189915564}, 'dropout1': 0.2393268964938784, 'dropout2': 0.25214494854877045, 'learn_rate': 0.0004474453000962455, 'units1': 133.21267496596215, 'units2': 99.60223386848833}\n",
      "Log-Loss:                                                                          \n",
      "0.016812002286314964                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.4994700603234974, 'layers': 'three', 'units3': 249.2555359930098}, 'dropout1': 0.3034745480694431, 'dropout2': 0.158145037841622, 'learn_rate': 0.00020544640831985408, 'units1': 80.85543528248081, 'units2': 155.15839148418624}\n",
      "Log-Loss:                                                                          \n",
      "0.016536550596356392                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 64, 'choice': {'dropout3': 0.2915342914454786, 'layers': 'three', 'units3': 176.88563493080335}, 'dropout1': 0.26237564814910164, 'dropout2': 0.11849889421742324, 'learn_rate': 0.00029517240781005814, 'units1': 112.25292733772937, 'units2': 135.84607572073017}\n",
      "Log-Loss:                                                                          \n",
      "0.016459988430142403                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.4102265322939815, 'layers': 'three', 'units3': 204.0278400537133}, 'dropout1': 0.12466060125390997, 'dropout2': 0.17899804341884673, 'learn_rate': 0.0006059378070539438, 'units1': 214.62326138351264, 'units2': 226.9813655367855}\n",
      "Log-Loss:                                                                          \n",
      "0.015919141471385956                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 128, 'choice': {'dropout3': 0.33773017980637154, 'layers': 'three', 'units3': 227.402782829917}, 'dropout1': 0.15444711547580875, 'dropout2': 0.27864787760589116, 'learn_rate': 0.00042449255544062183, 'units1': 126.29239024270409, 'units2': 171.15901870275434}\n",
      "Log-Loss:                                                                          \n",
      "0.016611840575933456                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.2537330740078031, 'layers': 'three', 'units3': 173.5705972756543}, 'dropout1': 0.30075692797997444, 'dropout2': 0.24471203535365482, 'learn_rate': 0.0005091192766969358, 'units1': 166.78957961374957, 'units2': 179.1491570076172}\n",
      "Log-Loss:                                                                          \n",
      "0.01565690152347088                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.2538967038879437, 'layers': 'three', 'units3': 138.35507351680457}, 'dropout1': 0.36773715126985096, 'dropout2': 0.22931804260993469, 'learn_rate': 0.0005037169021906373, 'units1': 164.58646575126082, 'units2': 235.3783068445561}\n",
      "Log-Loss:                                                                         \n",
      "0.015599983744323254                                                              \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 64, 'choice': {'dropout3': 0.25744227317168594, 'layers': 'three', 'units3': 141.20262213359547}, 'dropout1': 0.47099405364963853, 'dropout2': 0.23315581388927054, 'learn_rate': 0.00047608143740148515, 'units1': 170.54136752366355, 'units2': 236.03436340161426}\n",
      "Log-Loss:                                                                          \n",
      "0.016872283071279526                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 256, 'choice': {'dropout3': 0.1576170241498839, 'layers': 'three', 'units3': 116.45949683250582}, 'dropout1': 0.3603910101081083, 'dropout2': 0.2953814151581931, 'learn_rate': 0.0005967309054456845, 'units1': 191.2199526629958, 'units2': 215.51542252854958}\n",
      "Log-Loss:                                                                          \n",
      "0.018073713406920433                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.2463564600386221, 'layers': 'three', 'units3': 168.6762832350106}, 'dropout1': 0.4435810983511613, 'dropout2': 0.3223401166897427, 'learn_rate': 0.0005028405347409253, 'units1': 205.3915947173723, 'units2': 255.80495927995312}\n",
      "Log-Loss:                                                                          \n",
      "0.015754127874970436                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 128, 'choice': {'dropout3': 0.10207474719713727, 'layers': 'three', 'units3': 120.05421881218075}, 'dropout1': 0.31314985268954104, 'dropout2': 0.2610980089310465, 'learn_rate': 0.00034293946022133555, 'units1': 158.00974997909006, 'units2': 201.49884385923875}\n",
      "Log-Loss:                                                                          \n",
      "0.0174019206315279                                                                 \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.1904858455007189, 'layers': 'three', 'units3': 83.71932306091134}, 'dropout1': 0.38168819121845154, 'dropout2': 0.22761324727608495, 'learn_rate': 0.0006994669926033161, 'units1': 183.47719751262628, 'units2': 239.76082779690591}\n",
      "Log-Loss:                                                                          \n",
      "0.01567176543176174                                                                \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 32, 'choice': {'dropout3': 0.27055400564811966, 'layers': 'three', 'units3': 147.97908162517842}, 'dropout1': 0.42605697710645274, 'dropout2': 0.20479949423567265, 'learn_rate': 0.0007730275164025797, 'units1': 204.29186508055832, 'units2': 225.8774929541375}\n",
      "Log-Loss:                                                                          \n",
      "0.015651972964406013                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 256, 'choice': {'dropout3': 0.27747544843758265, 'layers': 'three', 'units3': 103.7138222201403}, 'dropout1': 0.4370331089409345, 'dropout2': 0.20585587179739523, 'learn_rate': 0.0009210305455983012, 'units1': 239.29868865143195, 'units2': 228.07284136117482}\n",
      "Log-Loss:                                                                          \n",
      "0.017509035766124725                                                               \n",
      "Params testing:                                                                    \n",
      "{'activation': 'relu', 'batch_size': 64, 'choice': {'dropout3': 0.3202271457384144, 'layers': 'three', 'units3': 153.5244025630578}, 'dropout1': 0.49392463931111086, 'dropout2': 0.16402917090023006, 'learn_rate': 0.0008642373694670114, 'units1': 209.54839565300384, 'units2': 208.95232327969177}\n",
      "Log-Loss:                                                                          \n",
      "0.015693051740527153                                                               \n",
      "100%|██████████| 50/50 [11:59<00:00, 14.40s/trial, best loss: 0.015599983744323254]\n",
      "best:  {'batch_size': 0, 'dropout1': 0.36773715126985096, 'dropout2': 0.22931804260993469, 'dropout3': 0.2538967038879437, 'learn_rate': 0.0005037169021906373, 'num_layers': 1, 'units1': 164.58646575126082, 'units2': 235.3783068445561, 'units3': 138.35507351680457}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with hyperopt library, seems working and fast.\n",
    "# see https://stackoverflow.com/questions/43533610/how-to-use-hyperopt-for-hyperparameter-optimization-of-keras-deep-learning-netwo\n",
    "# try to add batch_size and learning rate as hyperparameters.\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import sys\n",
    "\n",
    "space = {'choice': hp.choice('num_layers',\n",
    "                    [ {'layers':'two', },\n",
    "                    {'layers':'three',\n",
    "                    'units3': hp.uniform('units3', 64,256), \n",
    "                    'dropout3': hp.uniform('dropout3', .1,.5)}\n",
    "                    ]),\n",
    "\n",
    "            'units1': hp.uniform('units1', 64,256),\n",
    "            'units2': hp.uniform('units2', 64,256),\n",
    "\n",
    "            'dropout1': hp.uniform('dropout1', .1,.5),\n",
    "            'dropout2': hp.uniform('dropout2',  .1,.5),\n",
    "\n",
    "            'batch_size': hp.choice('batch_size', [32,64,128,256]),  #has to be int32, cannot use hp.uniform\n",
    "\n",
    "            'learn_rate': hp.uniform('learn_rate', 0.0001,0.001),\n",
    "            #'epochs' :  30,\n",
    "            #'optimizer': hp.choice('optimizer',['adadelta','adam','rmsprop']),\n",
    "            'activation': 'relu'\n",
    "        }\n",
    "\n",
    "def f_nn(params):   \n",
    "\n",
    "    print ('Params testing: ', params)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['units1'], input_dim = X_pca70_train.shape[1])) \n",
    "    model.add(Activation(params['activation']))\n",
    "    model.add(Dropout(params['dropout1']))\n",
    "\n",
    "    model.add(Dense(params['units2'])) \n",
    "    model.add(Activation(params['activation']))\n",
    "    model.add(Dropout(params['dropout2']))\n",
    "\n",
    "    if params['choice']['layers']== 'three':\n",
    "        model.add(Dense(params['choice']['units3'])) \n",
    "        model.add(Activation(params['activation']))\n",
    "        model.add(Dropout(params['choice']['dropout3']))    \n",
    "\n",
    "    model.add(Dense(206))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=params['learn_rate'], decay=0.001/200), loss=BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n",
    "\n",
    "    model.fit(X_pca70_train, y_train, epochs=30, batch_size=params['batch_size'], verbose = 0)\n",
    "\n",
    "#     pred_auc =model.predict_proba(X_pca70_val, batch_size = 32, verbose = 0)\n",
    "#     acc = roc_auc_score(y_val, pred_auc)\n",
    "#     print('AUC:', acc)\n",
    "#     sys.stdout.flush() \n",
    "#     return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "#     pred = model.predict(X_pca70_val, batch_size = 32, verbose = 0)\n",
    "#     logloss_ = logloss(y_val, pred)\n",
    "    score_logloss = model.evaluate(X_pca70_val, y_val, verbose=0)\n",
    "    print('Log-Loss:', score_logloss[1])\n",
    "    sys.stdout.flush() \n",
    "    return {'loss': score_logloss[1], 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(f_nn, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "print('best: ', best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-ireland",
   "metadata": {},
   "source": [
    "Fine-tuning the batch size (note, should be 32 not 0 if check back the output log) and learning rate is not better than previous tuning. Actually got batch size 32 and very similar learning rate. So really stop here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-being",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_latest_p37)",
   "language": "python",
   "name": "conda_tensorflow2_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
